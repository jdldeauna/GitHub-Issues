{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RMACC_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdldeauna/GitHub-Issues/blob/master/RMACC_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-58bMjMJfJY"
      },
      "source": [
        "# Fundamental Machine Learning Pipeline applied to Atmospheric Science Data\n",
        "\n",
        "In this tutorial, we will explore different aspects of the machine learning (ML) pipeline using weather data. This includes pre-processing techniques, how to split data for training\\validation\\testing the models, how to train basic ML models, and finally how to evaluate the model performance. \n",
        "\n",
        "I will go through lecture-like materials, with links to (hopefully) helpul resources, and coding exercises. \n",
        "\n",
        "Please cite the notebook as follows:\n",
        "\n",
        "    Burke, A., 2021: \"Fundamental Machine Learning Pipeline applied to Atmospheric Science Data\"\n",
        "\n",
        "The data for this tutorial can be cited as: \n",
        "\n",
        "    McGovern, A., Burke, A., Harrison, D., and G. M. Lackmann, 2020: A Machine Learning Tutorial for Operational Forecasting: Part I. Wea. Forecasting, In Press \n",
        "\n",
        "\n",
        "<br> \n",
        "<br> \n",
        "**My email: aburke1@ou.edu**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXbZ0rSzLm2O"
      },
      "source": [
        "<br>\n",
        "\n",
        "\n",
        "# Setup\n",
        "\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZaggDzOdqIJ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gdown\n",
        "! pip install netcdf4\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras.models as models\n",
        "import keras.layers as layers\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyv-DAec_vsT"
      },
      "source": [
        "### Prevent Auto-scrolling\n",
        "\n",
        "The next cell prevents output in the notebook from being nested in a scroll box"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1COjntx_v78"
      },
      "source": [
        "%%javascript\n",
        "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
        "    return false;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jURmN_9L1h0"
      },
      "source": [
        "## Import data\n",
        "\n",
        "The next cell imports all of the data that will be used by this notebook. If anything crashes, it will probably be here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjUYRwHhpGTr"
      },
      "source": [
        "! wget https://zenodo.org/record/4773839/files/AI_tutorial_data.tar.gz -O AI_tutorial_data.tar.gz\n",
        "! tar -xzvf AI_tutorial_data.tar.gz\n",
        "! rm *.tar.gz "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TdKzCbn9s7o"
      },
      "source": [
        "<br>\n",
        "\n",
        "\n",
        "# Tutorial Overview\n",
        "\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flI8YddqNYY6"
      },
      "source": [
        "\n",
        "**Brief:** \n",
        "1. General ML Pipeline\n",
        "2. Prediction Problem and Dataset Information\n",
        "\n",
        "**More in Depth:**\n",
        "3. Data Pre-processing\n",
        "4. Model Training\n",
        "  - Linear Regression\n",
        "  - Artificial Neural Network\n",
        "5. Model evaluation\n",
        "6. Exercise\n",
        "\n",
        "[Here](https://towardsdatascience.com/list-of-free-must-read-machine-learning-books-89576749d2ff) is a list of free books about ML from a statistical perspective.\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mskE_XWTuL3"
      },
      "source": [
        "<br>\n",
        "\n",
        "\n",
        "# **1. General ML pipeline**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEsVHrZTUghy"
      },
      "source": [
        "### The (brief) end-to-end process for using ML models\n",
        "\n",
        "*Define the Problem*\n",
        "  - Look at the data and forecasting task to see if ML is appropriate. Do existing methods produce skillful results? If so then ML may not be necessary.\n",
        "  - Is the data normally distributed, correlated, etc.? **Do you have enough data if what you are trying to predict is rare?**\n",
        "\n",
        "*Separate Data* \n",
        "  - Split your data into **at least two independent sets**: training and testing\n",
        "  - Possibly use validation set to tune ML models before applying them to the testing set. \n",
        "\n",
        "*Data Pre-processing* \n",
        "  - After determining ML would benefit your problem domain, transform the data to be more amenable for ML\n",
        "  - Find out what shape the data needs to be (vectors for ML versus tensors for DL)\n",
        "  - Normalize/Scale data\n",
        "  \n",
        "### ***The above steps are at least 80% of the work when working with ML models***\n",
        "\n",
        "\n",
        "*Model Training*\n",
        "  - Optimize ML model(s) using the **training dataset only**\n",
        "\n",
        "*Model Deployment*\n",
        "  - Apply the trained ML model to the **testing dataset only** \n",
        "\n",
        "*Model Evaluation*\n",
        "  - Use verification metrics and subjective evaluations to determine the skill of the ML model for a given predictive task.  \n",
        "\n",
        "*Model Interpretation*\n",
        "  - Use different interpretation techniques (permutation variable importance, partial dependence plots, backwards optimization, etc.) to determine **why** the ML model(s) make decisions\n",
        "  - Evaluate if the decision-making process is reasonable\n",
        "  - Allow end-users to better trust the ML decisions\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "## These steps are applicable for both ML and DL models. More details about the ML pipeline can be found [here](https://www.oreilly.com/library/view/building-machine-learning/9781492053187/ch01.html). \n",
        "\n",
        "<br>\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcMlMUTiZrD2"
      },
      "source": [
        "<br>\n",
        "\n",
        "\n",
        "# **2. Prediction Problem and Dataset Information**\n",
        "\n",
        "\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBCUBekJZwwP"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/NWS_Weather_Forecast_Offices.svg/720px-NWS_Weather_Forecast_Offices.svg.png\" width=\"600\">\n",
        "\n",
        "\n",
        "We are going to mimic the [WxChallenge](https://www.wxchallenge.com/) (with slight changes for time constraints)\n",
        "  - Predict the high/low temperature and wind speed of a given city\n",
        "  - Relatively straight forward problem that is familiar to many meteorologists \n",
        "  - Learn the ML process on one city and apply your knowledge to another city or an entirely different prediction problem, it is up to you! (See section 9) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hkjNQdl1_CY"
      },
      "source": [
        " **Dataset Information**\n",
        "- Numerical Weather Prediction (NWP) point forecasts from the Iowa Environmental Mesonet between 1 January 2011 to 1 August 2019\n",
        "- 24-hour forecast runs from the GFS, NAM, RAP, and NAM 4km at 0000, 0600, 1200, and 1800 UTC\n",
        "- 70 hourly forecast variables for each model \n",
        "- NMP variables processed into 24-hour maximum, minimum, and average values for the 0600 to 0600 UTC period (same as WxChallenge)\n",
        "- If one NWP forecast is missing, replaced with average value from all other NWP forecasts for the given variable\n",
        "- Observations are NWS ASOS point data for a given city  \n",
        "- Days with missing observations were removed\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K4wdvjQbVnD"
      },
      "source": [
        "<br> \n",
        "\n",
        "\n",
        "Load in the CSV data. *We will look at kdfw*\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMyg_Y90bV7e"
      },
      "source": [
        "total_dataset = pd.read_csv('AI_tutorial_data/kdfw_processed_data.csv',index_col=0).sort_values(by='date')\n",
        "total_dataset = total_dataset.replace('********', np.nan).replace(np.inf,np.nan).dropna(how='any',axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA3NPPrEoURz"
      },
      "source": [
        "#First five rows\n",
        "total_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PqOsKd8oUOe"
      },
      "source": [
        "#Last five rows\n",
        "total_dataset.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfWwf3UnbVHG"
      },
      "source": [
        "print('The columns (predictor/input variables) we have to work with are:\\n')\n",
        "total_dataset.columns\n",
        "\n",
        "#Explore the different variables if you wish"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXhiREAB9bI4"
      },
      "source": [
        "# **3. Data Pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRevmGVWDzgI"
      },
      "source": [
        "## Partitioning Data\n",
        "\n",
        "What would happen if we use all of our data to build a model and evaluate its accuracy?\n",
        "\n",
        "***Let's try!*** (Do not need to know how to do this yet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waIUkqG-FKvq"
      },
      "source": [
        "#Observation data\n",
        "total_label_data = total_dataset.filter(like='OBS')\n",
        "print(total_label_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rn-7OsDFLmw"
      },
      "source": [
        "#Variables used for training \n",
        "dropCols = list(total_label_data.columns) + ['date']\n",
        "total_feature_data = total_dataset.copy(deep=True)\n",
        "total_feature_data = total_feature_data.drop(dropCols,axis=1)\n",
        "print(total_feature_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdDajh_uFKqK"
      },
      "source": [
        "max_temperature_obs = total_label_data['OBS_tmpf_max'][:2000]\n",
        "first_50_features = total_feature_data.iloc[:2000,:50]\n",
        "\n",
        "# # Train and evaluate the model to predict max temperature\n",
        "linear_regression_model = LinearRegression()\n",
        "\n",
        "#Train\n",
        "linear_regression_model.fit(first_50_features.values,max_temperature_obs.values)\n",
        "# Evaluate R^2\n",
        "print(f'Testing data shape: {first_50_features.shape}')\n",
        "print(f'Observation data shape: {max_temperature_obs.shape}')\n",
        "score = linear_regression_model.score(first_50_features.values,max_temperature_obs.values)\n",
        "print(f'R squared score: {score}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aln0xTYtJLFG"
      },
      "source": [
        "What happens if we test on other data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjKzBK2FJJ2F"
      },
      "source": [
        "max_temperature_obs = total_label_data['OBS_tmpf_max'][-500:]\n",
        "first_50_features = total_feature_data.iloc[-500:,:50]\n",
        "\n",
        "score = linear_regression_model.score(first_50_features.values,max_temperature_obs.values)\n",
        "print(f'Testing data shape: {first_50_features.shape}')\n",
        "print(f'Observation data shape: {max_temperature_obs.shape}')\n",
        "print(f'R squared score: {score}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRLMp9FzJuii"
      },
      "source": [
        "### Dataset independence \n",
        "- **Critical** when deploying machine learning models. \n",
        "  - Training and evaluating on the same data gives an unrealistic view of how the model will perform on new data\n",
        "  - Instead, split the data into **training** and **testing** datasets that are independent enough to provide a clear picture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it6nyMj6TGsL"
      },
      "source": [
        "### How much data should go in each subset?\n",
        "  - Depends (I know, not the most ideal answer)\n",
        "  - Typical train/test split is 80% for training and 20% for testing\n",
        "    - Varies for the size of your dataset\n",
        "    - Possibly need to use data augmentation to increase training data size \n",
        "    - Find more information on data augmentation for deep learning [here](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/)\n",
        "\n",
        "### **Lets split the data officially into training and testing datasets** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1ALroM4LICd"
      },
      "source": [
        "# Function to split data based on given dates\n",
        "def split_data_year(input_data,labels,start_date_str,end_date_str):\n",
        "  data = input_data.copy()\n",
        "  date_list = pd.to_datetime(data['date'])\n",
        "  date_mask = (date_list > start_date_str) & (date_list <= end_date_str)\n",
        "  out_data = data.loc[date_mask,:].drop(['date'],axis=1)\n",
        "  out_labels = labels.loc[date_mask,:]\n",
        "  return out_data,out_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8meX2TT7Pkw4"
      },
      "source": [
        "# Remove MOS data based on previous work \n",
        "mosCols = [key for key in total_dataset.columns if 'MOS' in key]\n",
        "total_dataset = total_dataset.drop(mosCols, axis = 1)\n",
        "#Get total data\n",
        "total_label_data = total_dataset.filter(like='OBS')\n",
        "dropCols = list(total_label_data.columns)\n",
        "total_feature_data = total_dataset.copy(deep=True).drop(dropCols,axis=1)\n",
        "\n",
        "#Training data between 2011 and 2017\n",
        "train_features, train_labels = split_data_year(total_feature_data,\n",
        "    total_label_data,'2011-01-01','2017-12-31')\n",
        "\n",
        "#Testing data between 2018 and 2019\n",
        "#Keep a month between train/test data for greater likelihood \n",
        "# of dataset independence\n",
        "test_features, test_labels = split_data_year(total_feature_data,\n",
        "    total_label_data,'2018-02-01','2019-12-31')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVfCrMfOTZdd"
      },
      "source": [
        "## Transforming Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7l4eHCtTv99"
      },
      "source": [
        "### Normalization and Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7qkRpsbHRsf"
      },
      "source": [
        " \n",
        "Why should you scale your data?\n",
        "- Data variables with different scales (e.g. temperature versus sea-level pressure) affects what variables the ML models find important \n",
        "- Temperatures varying from $\\sim$180-330 K will be prioritized over specific humidity values varying from $\\sim$0-0.02 kg kg$^{-1}$\n",
        "\n",
        "Normalizing or scaling data removes this issue.\n",
        "\n",
        "**Important: We normalize/scale our data based on the training data and save these values to apply to testing data**\n",
        "\n",
        "More details about each transformation method can be found [here](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcRlVAs3HRkq"
      },
      "source": [
        "***MinMaxScaler***\n",
        "\n",
        "Scalar = $\\frac{x_{i} - min(x)}{max(x) - min(x)}$\n",
        "\n",
        "Scales data between 0 and 1 based on training data minimum and maximum values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp7WAuz3Ky6D"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Fit/find minimum and maximum value for each input variable\n",
        "min_max_model = MinMaxScaler().fit(train_features)\n",
        "\n",
        "# This saves the values to apply or 'transform' testing data\n",
        "min_max_train_features = min_max_model.transform(train_features)\n",
        "\n",
        "print('Training data range \\nMax:',np.nanmax(min_max_train_features),\n",
        "    ', Min:',np.nanmin(min_max_train_features))\n",
        "\n",
        "\n",
        "#################################\n",
        "# Apply the min/max values to testing data\n",
        "min_max_test_features = min_max_model.transform(test_features)\n",
        "\n",
        "print('\\nTesting data range \\nMax:',np.nanmax(min_max_test_features),\n",
        "    ', Min:',np.nanmin(min_max_test_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIY5Q7OiS5AV"
      },
      "source": [
        "### **The scaled data for the rest of the tutorial**\n",
        "\n",
        "**My preferred scaling method**\n",
        "\n",
        "- Replace any outlier values ( > 3 standard deviations from mean) with the training data mean\n",
        "\n",
        "- Use MinMaxScaler after replacement\n",
        "\n",
        "- Reduces the chance outliers affect training scaling values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FSoLMU5i1tw"
      },
      "source": [
        "no_outlier_train_features = train_features.copy()\n",
        "no_outlier_test_features = test_features.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiGxIQtjbfMs"
      },
      "source": [
        "train_standard_dev = train_features.std()\n",
        "train_mean = train_features.mean()\n",
        "\n",
        "for column in train_features.columns:\n",
        "  outlier_threshold_value = 3.0*train_standard_dev[column]\n",
        "  \n",
        "  #pandas.where() documentation:\n",
        "  #Where cond is True, keep the original value. Where False, replace with corresponding value from other\n",
        "  no_outlier_train_features[column].where(\n",
        "      np.abs(train_features[column]-train_mean[column]) < outlier_threshold_value, \n",
        "      train_mean[column],\n",
        "      inplace=True)\n",
        "  \n",
        "  no_outlier_test_features[column].where(\n",
        "      np.abs(test_features[column]-train_mean[column]) < outlier_threshold_value, \n",
        "      train_mean[column],\n",
        "      inplace=True)\n",
        "  \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "no_outlier_min_max_model = MinMaxScaler().fit(no_outlier_train_features)\n",
        "scaled_no_outlier_train_features = no_outlier_min_max_model.transform(no_outlier_train_features)\n",
        "scaled_no_outlier_test_features = no_outlier_min_max_model.transform(no_outlier_test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TULHoEiJSbYQ"
      },
      "source": [
        "If you only want the data for the rest of the tutorial, do not run the cell below (it will return an error).\n",
        "\n",
        "If you're going through the entire tutorial up until now, go ahead and run the cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtIM06Ppb_9O"
      },
      "source": [
        "print('Previous MinMaxScaler\\n')\n",
        "print(\n",
        "    f'Training data range \\nMax: {np.nanmax(min_max_train_features)}, '+\n",
        "    f'Min: {np.nanmin(min_max_train_features)}')\n",
        "print(\n",
        "    f'Testing data range \\nMax: {np.nanmax(min_max_test_features)}, '+\n",
        "    f'Min: {np.nanmin(min_max_test_features)}')\n",
        "\n",
        "print('\\n\\nNo outlier MinMaxScaler\\n')\n",
        "print(\n",
        "    f'Training data range \\nMax: {np.nanmax(scaled_no_outlier_train_features)}, '+\n",
        "    f'Min: {np.nanmin(scaled_no_outlier_train_features)}')\n",
        "print(\n",
        "    f'Testing data range \\nMax: {np.nanmax(scaled_no_outlier_test_features)}, '+\n",
        "    f'Min: {np.nanmin(scaled_no_outlier_test_features)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gblEdsLITv71"
      },
      "source": [
        "### Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cANrvN16ligh"
      },
      "source": [
        "\n",
        "**Why perform dimensionality reduction?**\n",
        "- Requires less space to store data\n",
        "- Model training is faster on smaller dimensional datasets\n",
        "- Only train on relevant features (reduces redundancy)\n",
        "- Easier to visualize in lower dimensional space\n",
        "\n",
        "<br>\n",
        "\n",
        "**Replacing outliers with mean training data values (previous scaling step) is crucial when using dimensionality reduction with the WxChallenge data**\n",
        "\n",
        "...We had to learn this the hard way!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ9pVo3L1C_d"
      },
      "source": [
        "#### Principal Component Analysis (PCA) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULJ10R351Egf"
      },
      "source": [
        "PCA reduces the dimensionality of a dataset by creating new variables that are linear combinations of the original features. This way, less variables are needed to encompass the variability of the original data and redundant information is reduced. \n",
        "\n",
        "<br> \n",
        "\n",
        "The *principal components (PCs)* or new variables are linearly independent and uncorrelated, which can be beneficial for certain model assumptions. The first PC is a new variable with the most variance from the original dataset, the second PC is a new variable with the second most variance, etc. \n",
        "\n",
        "\n",
        "<br> \n",
        "\n",
        "\n",
        "Scaling is really important so that the PCs do not emphasize variables simply because they have large data ranges.\n",
        "\n",
        "Find more information [here](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\n",
        ") and [here](\n",
        "https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L-Qv2BlHiLn"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/1280/1*mYuvXAmJmkqYuEoGsYCRwQ.gif\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdEtNnp5AfEe"
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H86oOFyHTvXO"
      },
      "source": [
        "print(f'Before PCA: \\n{scaled_no_outlier_train_features[0,:10]}')\n",
        "\n",
        "#Fit the PCA model with the already scaled data\n",
        "explorative_pca_model = PCA().fit(scaled_no_outlier_train_features)\n",
        "\n",
        "#Plot the cumulutive sum of the explained variance ratio \n",
        "plt.plot(np.cumsum(explorative_pca_model.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained variance')\n",
        "plt.grid(True)\n",
        "#If more than two components are wanted, fit the model again and then transform\n",
        "\n",
        "#Otherwise transform the desired data using a fitted PCA model\n",
        "exporative_pca_train_features = explorative_pca_model.transform(scaled_no_outlier_train_features)\n",
        "\n",
        "print('After PCA: \\n' + str(exporative_pca_train_features[0,:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODkq7FjFoG0G"
      },
      "source": [
        "Actual PCA model based on previous work, **experiment with 'n_components' value for different number of input features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGXf4zcWn7MD"
      },
      "source": [
        "pca_model = PCA(n_components = 0.65, svd_solver = 'full').fit(scaled_no_outlier_train_features)\n",
        "pca_train_features = pca_model.transform(scaled_no_outlier_train_features)\n",
        "pca_test_features = pca_model.transform(scaled_no_outlier_test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZpulRdUFRKe"
      },
      "source": [
        "pca_train_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B06Q2xVc887p"
      },
      "source": [
        "# **4. Model Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFemfh9LoeDN"
      },
      "source": [
        "## Linear Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyMWVbRTsjnc"
      },
      "source": [
        "Linear regression fits the following equation to the training data.\n",
        "\n",
        "$\\hat{y} = \\beta_0 + \\sum\\limits_{j = 1}^{M} \\beta_j x_j$\n",
        "\n",
        "$x$: Predictor variable\n",
        "\n",
        "$M$: Number of predictor variables\n",
        "\n",
        "$\\beta$: Coefficient for predictor variable\n",
        "\n",
        "$\\beta_0$: Bias coefficient/intercept\n",
        "\n",
        "$\\hat{y}$: Predicted label value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPqkWmhysoqn"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "The coefficient terms, $\\beta$ and $\\beta_0$ are adjusted during training to minimize the mean square error (MSE) between the predicted  label $\\hat{y}$ and true label $y$. \n",
        "\n",
        "$\\textrm{MSE} = \\frac{1}{N} \\sum\\limits_{i = 1}^{N} (\\hat{y}_i - y_i)^2$\n",
        "\n",
        "$N$: Number of training labels\n",
        "\n",
        "<br> \n",
        "\n",
        "Combining the two equations using the $\\hat{y}$ term yields:\n",
        "\n",
        "$\\textrm{MSE} = \\frac{1}{N} \\sum\\limits_{i = 1}^{N} (\\beta_0 + \\sum\\limits_{j = 1}^{M} \\beta_j x_{ij} - y_i)^2$\n",
        "\n",
        "**We want are $\\beta$ and $\\beta_0$ values that result in predicted values $\\hat{y}$  closest to the true label  ${y}$.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQRWYzv8soOX"
      },
      "source": [
        "<br>\n",
        "\n",
        "To do this, we take the derivative of each coefficient with respect to the MSE equation and minimize this value. This is why linear regression is sometimes referred to **least-squares linear regression**\n",
        "\n",
        "The derivatives of model coefficients with respect to MSE are as follows.\n",
        "\n",
        "$\\frac{\\partial}{\\partial \\beta_0}(\\textrm{MSE}) = \\frac{2}{N} \\sum\\limits_{i = 1}^{N} (\\hat{y}_i - y_i)$\n",
        "$\\frac{\\partial}{\\partial \\beta_j}(\\textrm{MSE}) = \\frac{2}{N} \\sum\\limits_{i = 1}^{N} x_{ij} (\\hat{y}_i - y_i)$\n",
        "\n",
        "The coefficients are updated throughout training by: \n",
        "\n",
        "$\\beta_0 \\leftarrow \\beta_0 - \\alpha \\frac{\\partial}{\\partial \\beta_0}(\\textrm{MSE})$\n",
        "\n",
        "$\\beta_j \\leftarrow \\beta_j - \\alpha \\frac{\\partial}{\\partial \\beta_j}(\\textrm{MSE})$\n",
        "\n",
        "\n",
        "$\\alpha$: Gradient descent parameter (how much do you want to change the coefficients at each update iteration)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSlNSOUyuD0K"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src=\"https://bs-uploads.toptal.io/blackfish-uploads/uploaded_file/file/238279/image-1587548990037-a9dc244b868173eb48df437eedc72910.gif\" width=\"600\">\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYRTi70vgsEQ"
      },
      "source": [
        "## Artificial Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGoAaH_tg1_O"
      },
      "source": [
        "The rest of the methods do not delve deeply into the inputs or outputs of a DL network. Instead they example the ***hidden layers*** and how each hidden neuron ***fires*** or ***activates***. \n",
        "\n",
        "<br>\n",
        "\n",
        "Neural networks are based on the processes behind neurons in the brain. If a certain neuron ***fires*** then information is passed along it. Otherwise  the information stops there. \n",
        "\n",
        "\n",
        "<img src=\"https://i.pinimg.com/originals/1c/63/b5/1c63b58aa68d9fee506e2397d05598e2.gif\" width=\"800\">\n",
        "\n",
        "[Image credit](https://www.dailymail.co.uk/sciencetech/article-2581184/The-dynamic-mind-Stunning-3D-glass-brain-shows-neurons-firing-real-time.html)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Similarly, only a hidden layer neuron that ***fires*** or ***activates*** passes along information.\n",
        "\n",
        "**NOTE:  There are a lot of good visualizations at https://distill.pub that I recommend looking at. I personally find them not only informative but fun as well.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFElStNLhoXy"
      },
      "source": [
        "Artificial Neural Network (ANN)​\n",
        "\n",
        "- Every data point associated with a label, no learning between points​\n",
        "\n",
        "- Removes any dependences in space or time​\n",
        "\n",
        "- Using a number of 'hidden layers' the network predicts based on the given data and then adjusts itself based on how wrong its prediction is to the truth​\n",
        "\n",
        "- Basically, a bunch of linear regressions sandwiched with non-linear *sparkle* ​\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1200/1*36MELEhgZsPFuzlZvObnxA.gif\" width=\"800\">\n",
        "\n",
        "\n",
        "​[Image credit](https://miro.medium.com/max/1200/1*36MELEhgZsPFuzlZvObnxA.gif)\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Facundo-Bre/publication/321259051/figure/fig1/AS:614329250496529@1523478915726/Artificial-neural-network-architecture-ANN-i-h-1-h-2-h-n-o.png\" width=\"800\">\n",
        "\n",
        "​[Image credit](https://www.researchgate.net/profile/Facundo-Bre/publication/321259051/figure/fig1/AS:614329250496529@1523478915726/Artificial-neural-network-architecture-ANN-i-h-1-h-2-h-n-o.png)\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WuMUxpzisae"
      },
      "source": [
        "In an artificial neural network, each neuron has inputs of weighted sums with a bias term added. \n",
        "- **Weights** are updated and *learned* based on how far off the networks prediction is from the true observation. \n",
        "- Same as gradient decent above. \n",
        "\n",
        "<img src=\"https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-3-42-21-am.png\" width=\"500\">\n",
        "\n",
        "​[Image credit](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-3-42-21-am.png)\n",
        "\n",
        "<br> \n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1200/1*ZafDv3VUm60Eh10OeJu1vw.png\" width=\"600\">\n",
        "\n",
        "\n",
        "​[Image credit](https://miro.medium.com/max/1200/1*ZafDv3VUm60Eh10OeJu1vw.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOUD-jQKtTfe"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACI4I3xTJugD"
      },
      "source": [
        "### **Validation**\n",
        "\n",
        "  - What if you don't want to use the python models out-of-the-box, but instead change the **hyperparameters** to better fit your data?\n",
        "\n",
        "  - Hyperparameters: user-defined parameters that constrain a model to speed up learning and/or prevent overfitting (McGovern et al. 2020) \n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/875/1*_7OPgojau8hkiPUiHoGK_w.png\" width=\"600\">\n",
        "\n",
        "\n",
        "  - Two (main) possibilities: \n",
        "    - Create a third independent subset for tuning hyperparameters, called  **validation dataset** \n",
        "    - Use cross-validation on the already divided training data if dataset is small"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e385jTupJuXk"
      },
      "source": [
        "\n",
        "### **Cross-Validation**\n",
        "  - Break up the training dataset into multiple **folds**, where a certain number are used to **fit** or **train** the ML model and a single fold (not used in training) **validates** the model (essentially testing the model)\n",
        "  - Repeat this procedure of breaking up training data into training/validation folds until every data fold is used for validation\n",
        "  - The trained ML model with the highest validation score of all the different training/validation pairs is evaluated on the testing data\n",
        "    - **Testing data is never used in training the model, only for evaluating a model already trained**\n",
        "\n",
        "\n",
        "The easiest way to do this in python is with [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UmCoCuoGi95"
      },
      "source": [
        "<img src=\"https://docs.google.com/uc?export=download&id=1Llru509goS8OA3irMXEmoCoDMDPVhUvS\" height= \"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWeYI0u1ua8E"
      },
      "source": [
        "### **Different Linear Regression Hyperparameters**\n",
        "\n",
        "\n",
        "From the [sklearn Linear Regression documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html): \n",
        " \n",
        "```\n",
        " class sklearn.linear_model.LinearRegression(*, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
        " ```\n",
        "\n",
        "Each of these can be changed when training the LinearRegression model.\n",
        "\n",
        "<br>\n",
        "\n",
        "No changes:\n",
        "\n",
        "```\n",
        "LinearRegression()\n",
        "```\n",
        "\n",
        "Allow the algorithm to calculate the intercept: \n",
        "\n",
        "```\n",
        "LinearRegression(fit_intercept=True)\n",
        "```\n",
        "\n",
        "Normalize the data before regression (*Usually false so a user can decide how to scale data*):\n",
        "```\n",
        "LinearRegression(normalize=False)\n",
        "```\n",
        "\n",
        "Copy the input feature data for training, else the data can be overwritten\n",
        "\n",
        "```\n",
        "LinearRegression(copy_X=True)\n",
        "```\n",
        "\n",
        "Train the model with multiple jobs/processes if n_jobs > 1 \n",
        "\n",
        "```\n",
        "LinearRegression(n_jobs=None)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHwxC3Acuqb9"
      },
      "source": [
        "## Fitting Linear Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiZvEL4-uqDn"
      },
      "source": [
        "Using our scaled data without outliers, train the LinearRegression (LinReg) model out-of-the-box (no change in hyperparameters)\n",
        "\n",
        "*We will have different LinReg models predicting the high temperature, low temperature, and wind speed.* \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_k7j5rdvTnc"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "linreg_model = LinearRegression()\n",
        "# High Temperature Model\n",
        "high_temp_linreg = linreg_model.fit(pca_train_features,train_labels['OBS_tmpf_max'].values)\n",
        "\n",
        "linreg_model = LinearRegression()\n",
        "# Low Temperature Model\n",
        "low_temp_linreg = linreg_model.fit(pca_train_features,train_labels['OBS_tmpf_min'].values)\n",
        "\n",
        "wind_linreg_model = LinearRegression()\n",
        "# Max Wind Speed Model\n",
        "wind_linreg = wind_linreg_model.fit(pca_train_features,train_labels['OBS_sknt_max'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-3A2fylkrB1"
      },
      "source": [
        "## Fitting Neural Network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqEY9I_3lKKZ"
      },
      "source": [
        "\n",
        "def ANN_WxChallenge(input_data,obs_data): \n",
        "  print( input_data.shape)\n",
        "  input_shape = (input_data.shape[1],)\n",
        "  print(input_shape)\n",
        "  # # Artificial Neural network\n",
        "  ann_model = models.Sequential()\n",
        "  ann_model.add(layers.GaussianNoise(0.01, input_shape=input_shape))\n",
        "\n",
        "  ann_model.add(layers.Dense(128))\n",
        "  ann_model.add(layers.BatchNormalization())\n",
        "  ann_model.add(layers.LeakyReLU(alpha=0.1))\n",
        "\n",
        "  ann_model.add(layers.Dense(64))\n",
        "  ann_model.add(layers.BatchNormalization())\n",
        "  ann_model.add(layers.LeakyReLU(alpha=0.1))\n",
        "\n",
        "  ann_model.add(layers.Dense(32))\n",
        "  ann_model.add(layers.BatchNormalization())\n",
        "  ann_model.add(layers.LeakyReLU(alpha=0.1))\n",
        "\n",
        "  ann_model.add(layers.Dense(1,activation='linear'))\n",
        "  print(ann_model.summary())\n",
        "\n",
        "  ann_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "  history = ann_model.fit(input_data, obs_data, epochs=20, batch_size=64)\n",
        "\n",
        "  return ann_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI1ctPWHnur0"
      },
      "source": [
        "high_temp_ann =  ANN_WxChallenge(scaled_no_outlier_train_features[:,:50],\n",
        "                                 train_labels['OBS_tmpf_max'].values)\n",
        "\n",
        "low_temp_ann =  ANN_WxChallenge(scaled_no_outlier_train_features[:,:50],\n",
        "                                 train_labels['OBS_tmpf_min'].values)\n",
        "\n",
        "wind_ann =  ANN_WxChallenge(scaled_no_outlier_train_features[:,:50],\n",
        "                                  train_labels['OBS_sknt_max'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-HcJryP88-9"
      },
      "source": [
        "# **5. Model Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y3wUUnK2xCB"
      },
      "source": [
        "Some common evaluation metrics used with regression problems are: \n",
        "\n",
        "Mean absolute error (MAE): $\\frac{1}{N} \\sum\\limits_{i = 1}^{N} \\lvert \\hat{y}_i - y_i \\rvert$\n",
        "\n",
        "Mean squared error (MSE): $\\frac{1}{N} \\sum\\limits_{i = 1}^{N} (\\hat{y}_i - y_i)^2$\n",
        "\n",
        "Mean signed error (\"bias\"): $\\frac{1}{N} \\sum\\limits_{i = 1}^{N} (\\hat{y}_i - y_i)$\n",
        "\n",
        "MAE skill score: $\\frac{\\textrm{MAE}_{\\textrm{climo}} - \\textrm{MAE}}{\\textrm{MAE}_{\\textrm{climo}}}$\n",
        "\n",
        "-  $\\textrm{MAE}_{\\textrm{climo}}$ is the average training MAE\n",
        "\n",
        "\n",
        "More evalution metrics can be found [here](https://scikit-learn.org/stable/modules/model_evaluation.html). \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KMc_bkEwbbn"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "high_temp_y_pred_linreg = high_temp_linreg.predict(pca_test_features)\n",
        "high_temp_mse_linreg = mean_squared_error(high_temp_y_pred_linreg,test_labels['OBS_tmpf_max'].values)\n",
        "\n",
        "print(f'Mean square error of Linear Regression: {high_temp_mse_linreg}')\n",
        "print(f'\\nPredicted values \\n{high_temp_y_pred_linreg[:10]}')\n",
        "print(f'\\nActual values \\n{test_labels[\"OBS_tmpf_max\"].values[:10] }')\n",
        "\n",
        "high_temp_y_pred_ann = high_temp_ann.predict(scaled_no_outlier_test_features[:,:50])\n",
        "high_temp_mse_ann = mean_squared_error(high_temp_y_pred_ann,test_labels['OBS_tmpf_max'].values)\n",
        "print(f'\\n\\nMean square error of ANN: {high_temp_mse_ann}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMeo4qn3Ot_h"
      },
      "source": [
        "Another type of verification/evaluation metric is the **reliability curve** \n",
        "- Shows model bias with respect to a conditional mean observation for each forecasted value\n",
        "- What is the mean observed value for a given range of forecast values\n",
        "- Data points above the 1-1 line are underforecasting, points under the line are overforecasting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzgkb5p3OtSe"
      },
      "source": [
        " def regression_reliability_curve(forecasts,observations,nbins=20):\n",
        "  step = (np.nanmax(forecasts)-np.nanmin(forecasts))/nbins\n",
        "  bins = np.arange(np.nanmin(forecasts),np.nanmax(forecasts),step)\n",
        "\n",
        "  mean_forecast = np.empty( (np.shape(bins)) )*np.nan\n",
        "  mean_obs = np.empty( (np.shape(bins)) )*np.nan\n",
        "\n",
        "  # For each bin, find the mean forecast value and observation\n",
        "  for b,bin in enumerate(bins[:-1]):\n",
        "    bin_indices = np.where((forecasts >= bin) & (forecasts < bins[b+1]))[0]\n",
        "    mean_forecast[b] = np.nanmean(forecasts[bin_indices])\n",
        "    mean_obs[b] = np.nanmean(observations[bin_indices])\n",
        "\n",
        "  #Get maximum value of both datasets to plot\n",
        "  max_value = np.nanmax( [np.nanmax(forecasts) , np.nanmax(observations)] )\n",
        "  perfect_values = np.arange(0.,max_value)\n",
        "\n",
        "  plt.figure(figsize=(5,5))\n",
        "  #Plotting 1-1 line\n",
        "  plt.plot(perfect_values,perfect_values,linestyle='dashed',color='k')\n",
        "  #Plotting mean forecast values versus the mean observations\n",
        "  plt.plot(mean_forecast,mean_obs)\n",
        "  plt.ylabel('Conditional Mean Observation',fontsize=16)\n",
        "  plt.xlabel('Forecast value',fontsize=16)\n",
        "  plt.title('Reliability Curve',fontsize=18)\n",
        "  plt.show()\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKzA3RtWSR5p"
      },
      "source": [
        "# Reliability curve of high temperature linear regression model\n",
        "regression_reliability_curve(high_temp_y_pred_linreg, test_labels['OBS_tmpf_max'].values, nbins=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHu_pU_t9HrG"
      },
      "source": [
        "# **6. Exercise**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOVUJxk-sOUw"
      },
      "source": [
        "---\n",
        "\n",
        "Select the city and data you would like to use. There are multiple WFO datapoints, to look at them run: \n",
        "\n",
        "```\n",
        "! ls AI_tutorial_data\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBn8Gy4zar6a"
      },
      "source": [
        "! ls AI_tutorial_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qevIWeJsK5o"
      },
      "source": [
        "data_file = 'AI_tutorial_data/kgeg_processed_data.csv'\n",
        "total_dataset = pd.read_csv(data_file,index_col=0).sort_values(by='date')\n",
        "\n",
        "\n",
        "###########################\n",
        "# Do not need to change the following lines\n",
        "###########################\n",
        "\n",
        "\n",
        "# Remove MOS data, unknown strings, and NaN values\n",
        "mosCols = [key for key in total_dataset.columns if 'MOS' in key]\n",
        "total_dataset = total_dataset.replace('********', np.nan).replace(np.inf,np.nan).dropna(how='any',axis=1).drop(mosCols, axis = 1)\n",
        "\n",
        "#Get label data\n",
        "total_label_data = total_dataset.filter(like='OBS')\n",
        "\n",
        "#Get feature data\n",
        "dropCols = list(total_label_data.columns)\n",
        "total_feature_data = total_dataset.copy(deep=True).drop(dropCols,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkla7HDws_nV"
      },
      "source": [
        "---\n",
        "\n",
        "Choose the train, validation, test split based on time periods\n",
        "\n",
        "See **Data Pre-processing > Partioning Data** Section\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RysTWdx3sKGF"
      },
      "source": [
        "# Function to split data based on given dates\n",
        "def split_data_year(input_data,labels,start_date_str,end_date_str):\n",
        "  data = input_data.copy()\n",
        "  date_list = pd.to_datetime(data['date'])\n",
        "  date_mask = (date_list > start_date_str) & (date_list <= end_date_str)\n",
        "  out_data = data.loc[date_mask,:].drop(['date'],axis=1)\n",
        "  out_labels = labels.loc[date_mask,:]\n",
        "  return out_data,out_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37XS3L5nt3Lm"
      },
      "source": [
        "###########################\n",
        "# Will need to change the rest of the lines in the exercise\n",
        "###########################\n",
        "\n",
        "\n",
        "#Training data \n",
        "train_features, train_labels = split_data_year(total_feature_data,total_label_data,\n",
        "      '2011-01-01','2017-12-31')\n",
        "\n",
        "# #Validation data\n",
        "# valid_features, valid_labels = split_data_year(total_feature_data,total_label_data,'','')\n",
        "\n",
        "#Testing data\n",
        "test_features, test_labels = split_data_year(total_feature_data,total_label_data,\n",
        "                                             '2018-02-01','2019-12-31')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgnWRxBKw8AU"
      },
      "source": [
        "---\n",
        "Apply transformations and Principal Component Analysis (optional) to data.\n",
        "\n",
        "*I highly suggest using PCA so that the ML models train in a timely manner*\n",
        "\n",
        "\n",
        "See  **Data Pre-processing > Normalization and Scaling** and **Data Pre-processing > Dimensionality Reduction > Principal Component Analysis** Sections. \n",
        "\n",
        "\n",
        "Other scaling tranformations [here](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpsr-RXhxQn7"
      },
      "source": [
        "# import PCA and/or metrics \n",
        "\n",
        "\n",
        "# First transform training data\n",
        "no_outlier_train_features = train_features.copy()\n",
        "no_outlier_test_features = test_features.copy()\n",
        "\n",
        "train_standard_dev = train_features.std()\n",
        "train_mean = train_features.mean()\n",
        "\n",
        "for column in train_features.columns:\n",
        "  outlier_threshold_value = 3.0*train_standard_dev[column]\n",
        "  \n",
        "  #pandas.where() documentation:\n",
        "  #Where cond is True, keep the original value. Where False, replace with corresponding value from other\n",
        "  no_outlier_train_features[column].where(\n",
        "      np.abs(train_features[column]-train_mean[column]) < outlier_threshold_value, \n",
        "      train_mean[column],\n",
        "      inplace=True)\n",
        "  \n",
        "  no_outlier_test_features[column].where(\n",
        "      np.abs(test_features[column]-train_mean[column]) < outlier_threshold_value, \n",
        "      train_mean[column],\n",
        "      inplace=True)\n",
        "  \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "no_outlier_min_max_model = MinMaxScaler().fit(no_outlier_train_features)\n",
        "scaled_no_outlier_train_features = no_outlier_min_max_model.transform(no_outlier_train_features)\n",
        "\n",
        "scaled_no_outlier_test_features = no_outlier_min_max_model.transform(no_outlier_test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDm-MYDhok7S"
      },
      "source": [
        "pca = PCA(n_components=0.65,svd_solver='full')\n",
        "\n",
        "pca.fit(scaled_no_outlier_train_features)\n",
        "pca_train_features = pca.transform(scaled_no_outlier_train_features)\n",
        "pca_test_features = pca.transform(scaled_no_outlier_test_features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXTYlHPRuq4l"
      },
      "source": [
        "----\n",
        "Tune model hyperparameters using validation data, could be cross-validation such as:\n",
        "```\n",
        "RandomizedSearchCV()\n",
        "```\n",
        "\n",
        "See **Model Training > Hyperparameter Tuning** Section\n",
        "\n",
        "\n",
        "CV function description [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2PAkdPBrXrB"
      },
      "source": [
        "#import CV function\n",
        "\n",
        "random_search = RandomizedSearchCV()\n",
        "random_search.fit(pca_valid_features,valid_labels) #validation data\n",
        "tuned_model_parameters = random_search.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8qM4H35vu8C"
      },
      "source": [
        "---\n",
        "Fit/Train ML models using the tuned model parameters and training data\n",
        "\n",
        "See **Model Training > Fitting Linear Regression Model** Section\n",
        "\n",
        "\n",
        "Information about different models [here](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning). \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVBKC2sXrbbd"
      },
      "source": [
        "#import model function\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf_model = RandomForestRegressor(n_estimators=150,max_depth=6)\n",
        "rf_model.fit(pca_train_features,train_labels['OBS_sknt_max'].values) #training data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcrO2rCkwmZz"
      },
      "source": [
        "---\n",
        "Evaluate model, change up dataset or training features if wanted\n",
        "\n",
        "See **Model Evaluation** Section\n",
        "\n",
        "Information about evalution metrics [here](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeF-RDpkrbKC"
      },
      "source": [
        "# import metrics \n",
        "\n",
        "pred_labels = rf_model.predict(pca_test_features) #testing data\n",
        "\n",
        "# Compare pred_labels to test_labels with metrics and/or reliability diagram\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DxglzQDyloD"
      },
      "source": [
        "# Reliability curve of high temperature linear regression model\n",
        "regression_reliability_curve(pred_labels, test_labels['OBS_sknt_max'].values, nbins=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWFPqVtBpua2"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "wind_mse = mean_squared_error(pred_labels,test_labels['OBS_sknt_max'].values)\n",
        "\n",
        "print(f'Mean square error: {wind_mse}')\n",
        "print(f'\\nPredicted values \\n{pred_labels[:10]}')\n",
        "\n",
        "print(f'\\nActual values \\n{test_labels[\"OBS_sknt_max\"].values[:10] }')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}